+++
abstract = "Crowdsourcing recently became a popular approach to substitute time consuming and expensive human subject studies, but its application is generally limited to simple and short-term experimental tasks, such as testing visual perception. The goal of this study is to test if crowdsourcing is applicable to a more complicated user study. Thus, we replicated a controlled lab study of decision-making tasks with different sorting techniques using crowdsourcing. A total of 98 participants were recruited via the Amazon Mechanical Turk service, and they participated in the study remotely through web interfaces. Experiment results indicate that performance measures of our crowdsourcing experiment was not exactly equivalent to lab experiments. However, we found potential sources of problems that we can improve to make the crowdsourcing experiment more viable."
abstract_short = "In this paper, we tested the efficacy of crowdsourcing on evaluating user's visualization task performances."
authors = ["Sung-Hee Kim", "Sensen Li", "Bum Chul Kwon", "Ji Soo Yi"]
date = "2011-10-01"
highlight = true
image_preview = ""
math = false
publication = "Proceedings of the Human Factors and Ergonomics Society Annual Meeting"
publication_short = "HFES"
publication_types = ["1"]
selected = false
title = "Investigating the Efficacy of Crowdsourcing on Evaluating Visual Decision Supporting System"
url_code = ""
url_dataset = ""
url_pdf = "pdf/crowdsourcing_efficacy.pdf"
url_project = ""
url_slides = ""
url_video = ""
url_bib = "bib/crowdsourcing_efficacy.bib"

#[[url_custom]]
#name = "Supplementary Material"
#url = "pdf/vlat_sup.pdf"

#[header]
#  caption = "The cognitive process that participants follow to answer a given investigative analysis question using a visual analytics tool."
#  image = "crowdsourcing_efficacy_teaser.png"

+++

